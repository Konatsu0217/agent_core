# MoE é£æ ¼çš„ Agent è·¯ç”±æ¶æ„è®¾è®¡

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

```
                    User Query
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Fast Decider    â”‚ â† è½»é‡çº§è·¯ç”±å™¨ï¼ˆå°æ¨¡å‹/è§„åˆ™ï¼‰
              â”‚  (è·¯ç”±å†³ç­–å™¨)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Fast Agent  â”‚          â”‚ Slow Agent  â”‚
    â”‚ (å¿«é€Ÿå“åº”)   â”‚          â”‚ (æ·±åº¦æ¨ç†)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â€¢ å•è½®è°ƒç”¨            â€¢ ReAct å¾ªç¯
    â€¢ å°æ¨¡å‹/ç¼“å­˜         â€¢ å¤šè½®å·¥å…·è°ƒç”¨
    â€¢ < 500ms            â€¢ 3-10 ç§’
```

---

## ğŸ“ å®Œæ•´æ¶æ„è®¾è®¡

### 1. æ•´ä½“ç»“æ„

```asiic
agent_core/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ orchestrator.py          # ä¸»å…¥å£
â”‚   â”œâ”€â”€ decider.py               # ğŸ”¥ NEW: å¿«é€Ÿè·¯ç”±å†³ç­–å™¨
â”‚   â”œâ”€â”€ fast_agent.py            # ğŸ”¥ NEW: å¿«é€Ÿå“åº” Agent
â”‚   â”œâ”€â”€ slow_agent.py            # ğŸ”¥ NEW: ReAct æ·±åº¦æ¨ç† Agent
â”‚   â””â”€â”€ agent_interface.py       # ğŸ”¥ NEW: Agent ç»Ÿä¸€æ¥å£
â”‚
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ llm_client.py            # æ”¯æŒå¤šæ¨¡å‹é…ç½®
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ models/
    â”œâ”€â”€ agent_request.py
    â”œâ”€â”€ agent_response.py
    â””â”€â”€ decider_result.py        # ğŸ”¥ NEW: å†³ç­–ç»“æœ
```

---

## ğŸš€ æ ¸å¿ƒæ¨¡å—è®¾è®¡

### Module 1: Agent Interface (ç»Ÿä¸€æ¥å£)

```python
# core/agent_interface.py
from abc import ABC, abstractmethod
from models.agent_request import AgentRequest
from models.agent_response import AgentResponse

class BaseAgent(ABC):
    """æ‰€æœ‰ Agent çš„ç»Ÿä¸€æ¥å£"""
    
    @abstractmethod
    async def process(self, request: AgentRequest) -> AgentResponse:
        """å¤„ç†ç”¨æˆ·è¯·æ±‚"""
        pass
    
    @abstractmethod
    def get_capabilities(self) -> dict:
        """è¿”å› Agent èƒ½åŠ›æè¿°ï¼ˆç”¨äº Decider å†³ç­–ï¼‰"""
        pass
    
    @abstractmethod
    def estimate_cost(self, request: AgentRequest) -> dict:
        """ä¼°ç®—å¤„ç†è¯¥è¯·æ±‚çš„æˆæœ¬ï¼ˆæ—¶é—´/Tokenï¼‰"""
        pass
```

---

### Module 2: Fast Agent (å¿«é€Ÿå“åº”)

```python
# core/fast_agent.py
from core.agent_interface import BaseAgent

class FastAgent(BaseAgent):
    """
    å¿«é€Ÿå“åº” Agent
    - ä½¿ç”¨å°æ¨¡å‹ï¼ˆå¦‚ GPT-3.5, Claude Haikuï¼‰
    - å•è½®è°ƒç”¨ï¼Œä¸ä½¿ç”¨å·¥å…·
    - é€‚åˆç®€å•é—®ç­”ã€é—²èŠã€å·²çŸ¥ä¿¡æ¯æŸ¥è¯¢
    """
    
    def __init__(self, settings: AgentSettings):
        self.llm_client = LLMClient(
            model="gpt-3.5-turbo",  # æˆ– claude-3-haiku
            temperature=0.7
        )
        self.context_cache = {}  # å†…ç½®ä¸Šä¸‹æ–‡ç¼“å­˜
    
    async def process(self, request: AgentRequest) -> AgentResponse:
        """å•è½®å¿«é€Ÿå“åº”"""
        # 1. æ£€æŸ¥ç¼“å­˜
        if cache_hit := self._check_cache(request.user_query):
            return cache_hit
        
        # 2. æ„å»ºç®€å• prompt
        messages = self._build_simple_messages(request)
        
        # 3. å•æ¬¡ LLM è°ƒç”¨
        response = await self.llm_client.chat_completion(messages)
        
        # 4. ç¼“å­˜ç»“æœ
        self._cache_response(request.user_query, response)
        
        return AgentResponse(
            request_id=request.request_id,
            text_output=response["choices"][0]["message"]["content"],
            agent_type="fast",
            metadata={
                "latency_ms": 300,
                "tokens": 150,
                "cached": False
            }
        )
    
    def get_capabilities(self) -> dict:
        return {
            "type": "fast",
            "can_use_tools": False,
            "max_complexity": "simple",
            "avg_latency_ms": 300,
            "good_for": [
                "é—²èŠ", "ç®€å•é—®ç­”", "å·²çŸ¥äº‹å®æŸ¥è¯¢",
                "ç¤¼è²Œæ€§å›å¤", "æƒ…æ„Ÿå›åº”"
            ]
        }
    
    def estimate_cost(self, request: AgentRequest) -> dict:
        return {
            "time_ms": 300,
            "tokens": 150,
            "price_usd": 0.0002
        }
    
    def _build_simple_messages(self, request: AgentRequest) -> list:
        """æ„å»ºä¸å¸¦å·¥å…·çš„ç®€å•æ¶ˆæ¯"""
        return [
            {
                "role": "system",
                "content": self.context_cache.get("system_prompt", 
                    "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œæä¾›ç®€æ´å‡†ç¡®çš„å›ç­”ã€‚")
            },
            {
                "role": "user",
                "content": request.user_query
            }
        ]
```

---

### Module 3: Slow Agent (æ·±åº¦æ¨ç†)

```python
# core/slow_agent.py
from core.agent_interface import BaseAgent
from handlers.tool_call_handler import ToolCallHandler

class SlowAgent(BaseAgent):
    """
    æ·±åº¦æ¨ç† Agent (ReAct)
    - ä½¿ç”¨å¼ºæ¨¡å‹ï¼ˆGPT-4, Claude Opus/Sonnetï¼‰
    - å¤šè½®å·¥å…·è°ƒç”¨
    - é€‚åˆå¤æ‚ä»»åŠ¡ã€éœ€è¦æ¨ç†å’Œè§„åˆ’çš„åœºæ™¯
    """
    
    def __init__(self, settings: AgentSettings):
        self.llm_client = LLMClient(
            model="gpt-4-turbo",  # æˆ– claude-3-5-sonnet
            temperature=0.7
        )
        self.tool_handler = ToolCallHandler(settings)
        self.pe_client = PEClient(settings)
        self.mcp_client = MCPClient(settings)
    
    async def process(self, request: AgentRequest) -> AgentResponse:
        """ReAct å¤šè½®æ¨ç†"""
        # 1. è°ƒç”¨ PE æ„å»ºå®Œæ•´è¯·æ±‚ï¼ˆåŒ…å«å·¥å…·ã€RAGï¼‰
        llm_request = await self.pe_client.build_request(
            session_id=request.session_id,
            user_query=request.user_query
        )
        
        # 2. ReAct å¾ªç¯
        messages = llm_request["messages"]
        tools = llm_request["tools"]
        
        max_iterations = 5
        iteration_logs = []
        
        for i in range(max_iterations):
            # Thought + Action
            response = await self.llm_client.chat_completion(
                messages=messages,
                tools=tools
            )
            
            iteration_logs.append({
                "iteration": i + 1,
                "thought": response.get("content"),
                "tool_calls": response.get("tool_calls")
            })
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            if not response.get("tool_calls"):
                break
            
            # Observation: æ‰§è¡Œå·¥å…·
            tool_results = await self.tool_handler.execute_tools(
                response["tool_calls"]
            )
            
            # æ›´æ–° messages
            messages.append({
                "role": "assistant",
                "content": response.get("content"),
                "tool_calls": response["tool_calls"]
            })
            
            for result in tool_results:
                messages.append({
                    "role": "tool",
                    "tool_call_id": result["id"],
                    "content": json.dumps(result["output"])
                })
        
        return AgentResponse(
            request_id=request.request_id,
            text_output=response["choices"][0]["message"]["content"],
            agent_type="slow",
            metadata={
                "iterations": i + 1,
                "total_tokens": sum([log["tokens"] for log in iteration_logs]),
                "latency_ms": 5000,
                "iteration_logs": iteration_logs
            }
        )
    
    def get_capabilities(self) -> dict:
        return {
            "type": "slow",
            "can_use_tools": True,
            "max_complexity": "high",
            "avg_latency_ms": 5000,
            "good_for": [
                "å¤æ‚æ¨ç†", "å¤šæ­¥éª¤ä»»åŠ¡", "éœ€è¦å·¥å…·è°ƒç”¨",
                "æ•°æ®åˆ†æ", "ä»£ç ç”Ÿæˆ", "æ·±åº¦é—®ç­”"
            ]
        }
    
    def estimate_cost(self, request: AgentRequest) -> dict:
        return {
            "time_ms": 5000,
            "tokens": 2000,
            "price_usd": 0.02
        }
```

---

### Module 4: Fast Decider (å¿«é€Ÿè·¯ç”±å™¨) âš¡

è¿™æ˜¯æœ€å…³é”®çš„éƒ¨åˆ†ï¼éœ€è¦**æå¿«çš„å†³ç­–é€Ÿåº¦**ã€‚

#### æ–¹æ¡ˆ A: è§„åˆ™ + è½»é‡çº§åˆ†ç±»ï¼ˆæ¨èï¼‰

```python
# core/decider.py
import re
from typing import Literal

AgentType = Literal["fast", "slow"]

class FastDecider:
    """
    å¿«é€Ÿè·¯ç”±å†³ç­–å™¨
    - ä¼˜å…ˆä½¿ç”¨è§„åˆ™åŒ¹é…ï¼ˆ0msï¼‰
    - è§„åˆ™æ— æ³•åˆ¤æ–­æ—¶è°ƒç”¨è½»é‡çº§åˆ†ç±»å™¨ï¼ˆ50-100msï¼‰
    """
    
    def __init__(self, settings: AgentSettings):
        self.settings = settings
        # ä½¿ç”¨æœ€å¿«çš„å°æ¨¡å‹åšåˆ†ç±»
        self.classifier_llm = LLMClient(
            model="gpt-3.5-turbo",  # æˆ–è€…æœ¬åœ° LLaMA 3B
            temperature=0.0,  # ç¡®å®šæ€§è¾“å‡º
            max_tokens=10  # åªéœ€è¦è¿”å› "fast" æˆ– "slow"
        )
        
        # è§„åˆ™æ¨¡å¼
        self.fast_patterns = [
            r"^(ä½ å¥½|hi|hello|å—¨)",  # é—®å€™
            r"(è°¢è°¢|æ„Ÿè°¢)",           # æ„Ÿè°¢
            r"^(ä»€ä¹ˆæ˜¯|define|è§£é‡Šä¸€ä¸‹)\s*[\w\u4e00-\u9fa5]{1,10}$",  # ç®€å•å®šä¹‰
            r"^(ä»Šå¤©|å¤©æ°”|æ—¥æœŸ|æ—¶é—´)",  # å¸¸è§æŸ¥è¯¢
        ]
        
        self.slow_patterns = [
            r"(å¸®æˆ‘|è¯·|ååŠ©).*(åˆ†æ|ç”Ÿæˆ|åˆ›å»º|è§„åˆ’)",  # å¤æ‚ä»»åŠ¡
            r"(æœç´¢|æŸ¥æ‰¾|è°ƒæŸ¥).*å¹¶.*",  # å¤šæ­¥éª¤
            r"(å¦‚æœ|å‡è®¾|å½“).*é‚£ä¹ˆ.*",  # æ¡ä»¶æ¨ç†
        ]
        
        # å…³é”®è¯æƒé‡
        self.fast_keywords = {
            "ä½ å¥½": 10, "è°¢è°¢": 10, "ä»€ä¹ˆæ˜¯": 8, "å¤©æ°”": 7,
            "æ—¶é—´": 7, "æ—¥æœŸ": 7, "å†è§": 10
        }
        
        self.slow_keywords = {
            "åˆ†æ": 8, "è§„åˆ’": 9, "æ¯”è¾ƒ": 7, "ç”Ÿæˆ": 8,
            "æœç´¢": 6, "è°ƒæŸ¥": 7, "è®¡ç®—": 6, "æ¨è": 7
        }
    
    async def decide(self, request: AgentRequest) -> tuple[AgentType, dict]:
        """
        å†³ç­–ä½¿ç”¨å“ªä¸ª Agent
        è¿”å›: (agent_type, metadata)
        """
        query = request.user_query.strip()
        
        # Stage 1: å¿«é€Ÿè§„åˆ™åŒ¹é… (0ms)
        if rule_result := self._rule_based_decision(query):
            return rule_result, {
                "method": "rule",
                "latency_ms": 0,
                "confidence": 0.95
            }
        
        # Stage 2: å…³é”®è¯æƒé‡æ‰“åˆ† (< 1ms)
        if score_result := self._keyword_scoring(query):
            return score_result, {
                "method": "keyword_scoring",
                "latency_ms": 1,
                "confidence": 0.85
            }
        
        # Stage 3: è½»é‡çº§ LLM åˆ†ç±» (50-100ms)
        llm_result = await self._llm_classification(query)
        return llm_result, {
            "method": "llm_classification",
            "latency_ms": 80,
            "confidence": 0.90
        }
    
    def _rule_based_decision(self, query: str) -> AgentType | None:
        """åŸºäºæ­£åˆ™è§„åˆ™çš„å¿«é€Ÿåˆ¤æ–­"""
        # æ£€æŸ¥ Fast æ¨¡å¼
        for pattern in self.fast_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return "fast"
        
        # æ£€æŸ¥ Slow æ¨¡å¼
        for pattern in self.slow_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return "slow"
        
        # é•¿åº¦å¯å‘å¼
        if len(query) < 20 and "?" not in query:
            return "fast"
        
        return None
    
    def _keyword_scoring(self, query: str) -> AgentType | None:
        """å…³é”®è¯æƒé‡æ‰“åˆ†"""
        fast_score = sum(
            weight for keyword, weight in self.fast_keywords.items()
            if keyword in query
        )
        
        slow_score = sum(
            weight for keyword, weight in self.slow_keywords.items()
            if keyword in query
        )
        
        # æ˜ç¡®å€¾å‘æ—¶è¿”å›
        if fast_score > slow_score + 5:
            return "fast"
        elif slow_score > fast_score + 5:
            return "slow"
        
        return None
    
    async def _llm_classification(self, query: str) -> AgentType:
        """è½»é‡çº§ LLM åˆ†ç±»ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰"""
        classification_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†ç±»å™¨ã€‚åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·è¯·æ±‚åº”è¯¥ç”¨å¿«é€Ÿå“åº”è¿˜æ˜¯æ·±åº¦æ¨ç†ï¼š

å¿«é€Ÿå“åº” (fast)ï¼šç®€å•é—®ç­”ã€é—²èŠã€å·²çŸ¥äº‹å®æŸ¥è¯¢
æ·±åº¦æ¨ç† (slow)ï¼šéœ€è¦å·¥å…·è°ƒç”¨ã€å¤šæ­¥éª¤æ¨ç†ã€å¤æ‚åˆ†æ

ç”¨æˆ·è¯·æ±‚: {query}

åªå›å¤ä¸€ä¸ªè¯: "fast" æˆ– "slow"
"""
        
        response = await self.classifier_llm.chat_completion(
            messages=[{"role": "user", "content": classification_prompt}],
            max_tokens=10,
            temperature=0.0
        )
        
        result = response["choices"][0]["message"]["content"].strip().lower()
        return "fast" if "fast" in result else "slow"
```

#### æ–¹æ¡ˆ B: æœ¬åœ°å°æ¨¡å‹åˆ†ç±»ï¼ˆæ›´å¿«ä½†éœ€è¦è®­ç»ƒï¼‰

```python
class LocalDecider:
    """
    ä½¿ç”¨æœ¬åœ°å°æ¨¡å‹ï¼ˆå¦‚ DistilBERTï¼‰åšåˆ†ç±»
    - æ¨ç†é€Ÿåº¦: 5-20ms
    - éœ€è¦é¢„å…ˆè®­ç»ƒæˆ–ä½¿ç”¨ Few-shot æç¤º
    """
    
    def __init__(self):
        from transformers import pipeline
        self.classifier = pipeline(
            "text-classification",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=0  # GPU
        )
    
    async def decide(self, query: str) -> AgentType:
        # æœ¬åœ°æ¨ç†ï¼Œæ— ç½‘ç»œå»¶è¿Ÿ
        result = self.classifier(query)
        # éœ€è¦å°† label æ˜ å°„åˆ° fast/slow
        return "fast" if result[0]["label"] == "SIMPLE" else "slow"
```

---

### Module 5: Orchestrator (ä¸»æ§åˆ¶å™¨)

```python
# core/orchestrator.py
from core.decider import FastDecider
from core.fast_agent import FastAgent
from core.slow_agent import SlowAgent

class AgentOrchestrator:
    """ä¸»è°ƒåº¦å™¨ï¼šMoE é£æ ¼è·¯ç”±"""
    
    def __init__(self, settings: AgentSettings):
        self.settings = settings
        
        # åˆå§‹åŒ– Decider å’Œ Agents
        self.decider = FastDecider(settings)
        self.fast_agent = FastAgent(settings)
        self.slow_agent = SlowAgent(settings)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "fast_count": 0,
            "slow_count": 0,
            "avg_fast_latency": 0,
            "avg_slow_latency": 0
        }
    
    async def process_query(self, request: AgentRequest) -> AgentResponse:
        """ä¸»å…¥å£ï¼šè·¯ç”± + æ‰§è¡Œ"""
        start_time = time.time()
        
        # 1. å¿«é€Ÿå†³ç­–ï¼ˆ< 100msï¼‰
        agent_type, decision_meta = await self.decider.decide(request)
        
        logger.info(
            "Agent decision made",
            request_id=request.request_id,
            agent_type=agent_type,
            decision_method=decision_meta["method"],
            decision_latency_ms=decision_meta["latency_ms"]
        )
        
        # 2. è·¯ç”±åˆ°å¯¹åº” Agent
        if agent_type == "fast":
            response = await self.fast_agent.process(request)
            self.stats["fast_count"] += 1
        else:
            response = await self.slow_agent.process(request)
            self.stats["slow_count"] += 1
        
        # 3. æ·»åŠ è·¯ç”±å…ƒä¿¡æ¯
        response.metadata["routing"] = {
            "agent_type": agent_type,
            "decision_meta": decision_meta,
            "total_latency_ms": (time.time() - start_time) * 1000
        }
        
        return response
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | Fast Agent | Slow Agent |
|------|-----------|-----------|
| **æ¨¡å‹** | GPT-3.5 / Claude Haiku | GPT-4 / Claude Sonnet |
| **å»¶è¿Ÿ** | 200-500ms | 3-10 ç§’ |
| **Token æ¶ˆè€—** | 100-300 | 1000-5000 |
| **æˆæœ¬** | $0.0002 | $0.02 |
| **å·¥å…·è°ƒç”¨** | âŒ | âœ… |
| **é€‚ç”¨åœºæ™¯** | ç®€å•é—®ç­”ã€é—²èŠ | å¤æ‚æ¨ç†ã€å¤šæ­¥éª¤ |

---

## ğŸ¯ Decider å†³ç­–é€»è¾‘å¯è§†åŒ–

```
User Query: "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: è§„åˆ™åŒ¹é…    â”‚
â”‚ Pattern: ^(ä½ å¥½|hi) â”‚ âœ… åŒ¹é…ï¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    è¿”å›: "fast" (0ms)


User Query: "å¸®æˆ‘åˆ†æä¸€ä¸‹æœ€è¿‘AIé¢†åŸŸçš„å‘å±•è¶‹åŠ¿ï¼Œå¹¶ç»™å‡ºæŠ•èµ„å»ºè®®"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: è§„åˆ™åŒ¹é…    â”‚ âŒ æ— åŒ¹é…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: å…³é”®è¯æ‰“åˆ†  â”‚
â”‚ slow_score: 17      â”‚ âœ… slow > fast + 5
â”‚ (åˆ†æ:8 + æŠ•èµ„:9)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    è¿”å›: "slow" (< 1ms)


User Query: "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: è§„åˆ™åŒ¹é…    â”‚ âŒ æ— åŒ¹é…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: å…³é”®è¯æ‰“åˆ†  â”‚ âŒ åˆ†æ•°æ¥è¿‘
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: LLM åˆ†ç±»   â”‚
â”‚ Model: GPT-3.5-turboâ”‚
â”‚ Prompt: "åˆ¤æ–­ä»»åŠ¡..." â”‚ âœ… è¿”å› "fast"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    è¿”å›: "fast" (80ms)
```

---

## ğŸš€ å®ç°æ­¥éª¤

### Phase 1: åŸºç¡€æ¶æ„ï¼ˆ1-2 å¤©ï¼‰
- [ ] åˆ›å»º `agent_interface.py`
- [ ] å®ç° `FastAgent` åŸºç¡€ç‰ˆæœ¬ï¼ˆå•è½®è°ƒç”¨ï¼‰
- [ ] å®ç° `SlowAgent` åŸºç¡€ç‰ˆæœ¬ï¼ˆå¤ç”¨ç°æœ‰ ReAct é€»è¾‘ï¼‰
- [ ] æ›´æ–° `Orchestrator` æ”¯æŒè·¯ç”±

### Phase 2: ç®€å• Deciderï¼ˆ0.5 å¤©ï¼‰
- [ ] å®ç° `FastDecider._rule_based_decision()`
- [ ] æ·»åŠ  10-20 æ¡è§„åˆ™æ¨¡å¼
- [ ] æµ‹è¯•è§„åˆ™è¦†ç›–ç‡

### Phase 3: å¢å¼º Deciderï¼ˆ1 å¤©ï¼‰
- [ ] å®ç° `_keyword_scoring()`
- [ ] å®ç° `_llm_classification()` ä½œä¸ºå…œåº•
- [ ] æ·»åŠ å†³ç­–æ—¥å¿—å’Œç»Ÿè®¡

### Phase 4: ä¼˜åŒ–ï¼ˆ1-2 å¤©ï¼‰
- [ ] FastAgent æ·»åŠ ç¼“å­˜æœºåˆ¶
- [ ] æ”¶é›†çœŸå®æŸ¥è¯¢ï¼Œä¼˜åŒ–è§„åˆ™
- [ ] A/B æµ‹è¯•ä¸åŒå†³ç­–ç­–ç•¥

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### 1. ç¼“å­˜ç­–ç•¥
```python
# FastAgent ä¸­
class FastAgent:
    def __init__(self):
        self.cache = TTLCache(maxsize=1000, ttl=3600)  # 1å°æ—¶è¿‡æœŸ
    
    def _check_cache(self, query: str) -> AgentResponse | None:
        # ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä½¿ç”¨ embeddingï¼‰
        query_embedding = self.embed(query)
        for cached_query, response in self.cache.items():
            if cosine_similarity(query_embedding, cached_query) > 0.95:
                return response
        return None
```

### 2. åŠ¨æ€é˜ˆå€¼è°ƒæ•´
```python
# Decider æ ¹æ®å†å²è¡¨ç°åŠ¨æ€è°ƒæ•´
class AdaptiveDecider(FastDecider):
    def __init__(self):
        super().__init__()
        self.fast_success_rate = 0.85  # FastAgent æˆåŠŸç‡
        self.slow_success_rate = 0.95
    
    def _adjust_threshold(self):
        # å¦‚æœ FastAgent è¡¨ç°å¥½ï¼Œé™ä½ slow é˜ˆå€¼
        if self.fast_success_rate > 0.90:
            self.slow_threshold += 1
```

### 3. æ··åˆç­–ç•¥
```python
# å¯¹äºæ¨¡ç³Šæƒ…å†µï¼Œå…ˆç”¨ Fast è¯•æ¢
class HybridOrchestrator(AgentOrchestrator):
    async def process_query(self, request: AgentRequest):
        agent_type, confidence = await self.decider.decide(request)
        
        if agent_type == "fast" and confidence < 0.8:
            # ç½®ä¿¡åº¦ä½æ—¶ï¼Œå…ˆå¿«é€Ÿå°è¯•
            fast_response = await self.fast_agent.process(request)
            
            # æ£€æŸ¥è´¨é‡
            if self._is_response_good(fast_response):
                return fast_response
            else:
                # é™çº§åˆ° Slow Agent
                return await self.slow_agent.process(request)
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

å»ºè®®åœ¨ `Orchestrator` ä¸­è·Ÿè¸ªè¿™äº›æŒ‡æ ‡ï¼š

```python
class Metrics:
    # è·¯ç”±åˆ†å¸ƒ
    fast_ratio: float  # Fast Agent ä½¿ç”¨æ¯”ä¾‹
    slow_ratio: float
    
    # æ€§èƒ½
    avg_decision_latency_ms: float  # å†³ç­–å»¶è¿Ÿ
    avg_fast_latency_ms: float
    avg_slow_latency_ms: float
    
    # è´¨é‡
    fast_user_satisfaction: float  # ç”¨æˆ·åé¦ˆ
    slow_user_satisfaction: float
    fast_fallback_rate: float  # Fast é™çº§åˆ° Slow çš„æ¯”ä¾‹
```

---

## ğŸ“ æµ‹è¯•ç”¨ä¾‹

```python
# tests/test_decider.py
test_cases = [
    # (query, expected_agent, max_latency_ms)
    ("ä½ å¥½", "fast", 5),
    ("è°¢è°¢ä½ çš„å¸®åŠ©", "fast", 5),
    ("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "fast", 100),
    ("ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", "fast", 100),
    
    ("å¸®æˆ‘åˆ†æè¿™ä»½æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š", "slow", 100),
    ("æœç´¢æœ€è¿‘AIè®ºæ–‡å¹¶æ€»ç»“è¦ç‚¹", "slow", 100),
    ("å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œå¸®æˆ‘è°ƒæ•´è¡Œç¨‹", "slow", 100),
]

@pytest.mark.asyncio
async def test_decider_accuracy():
    decider = FastDecider(settings)
    
    for query, expected, max_latency in test_cases:
        start = time.time()
        result, meta = await decider.decide(AgentRequest(query=query))
        latency = (time.time() - start) * 1000
        
        assert result == expected, f"Query '{query}' routed to {result}, expected {expected}"
        assert latency < max_latency, f"Decision took {latency}ms, expected < {max_latency}ms"
```

---

## âœ… æ€»ç»“

ä½ çš„ MoE æ€è·¯éå¸¸å¥½ï¼å…³é”®ç‚¹ï¼š

1. **Decider å¿…é¡»æå¿«** (< 100ms)ï¼šä¼˜å…ˆç”¨è§„åˆ™ï¼Œå…œåº•ç”¨è½»é‡çº§ LLM
2. **FastAgent å¤„ç† 80% çš„ç®€å•è¯·æ±‚**ï¼šèŠ‚çœæˆæœ¬å’Œæ—¶é—´
3. **SlowAgent å¤„ç† 20% çš„å¤æ‚ä»»åŠ¡**ï¼šä¿è¯è´¨é‡
4. **ç›‘æ§å’Œè¿­ä»£**ï¼šæ ¹æ®çœŸå®æ•°æ®ä¼˜åŒ–è·¯ç”±è§„åˆ™

è¿™ä¸ªæ¶æ„åœ¨å®é™…ç”Ÿäº§ä¸­éå¸¸å®ç”¨ï¼Œå¾ˆå¤šå…¬å¸éƒ½åœ¨ç”¨ç±»ä¼¼æ–¹æ¡ˆï¼ğŸš€

éœ€è¦æˆ‘å¸®ä½ å®ç°æŸä¸ªå…·ä½“æ¨¡å—å—ï¼Ÿæ¯”å¦‚ `FastDecider` çš„å®Œæ•´ä»£ç ï¼Ÿ